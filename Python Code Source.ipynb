{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd5d6f99-2df7-4b12-9bb0-764e0e77cf52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CNN+MLP (Proposed Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f10c0-4e44-45e9-a00c-1ee7e72e2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL CODE\n",
    "# \"Machine Learning Framework for Speech Intelligibility Prediction using Binaural Room Impulse Responses\"\n",
    "# Alfian et al., 2026\n",
    "# ============================================================\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import keras_tuner as kt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATA_ROOT = \"/content/drive/MyDrive/DATASET V3\"\n",
    "classes = {\"High Intelligibility\": 1, \"Low Intelligibility\": 0}\n",
    "SPLITS  = [\"training\", \"validation\", \"testing\"]\n",
    "\n",
    "T_EARLY_END  = 0.050\n",
    "T_LATE_START = 0.050\n",
    "T_TOTAL      = 3.0\n",
    "\n",
    "SR_TARGET = 22050\n",
    "N_FFT     = 1024\n",
    "HOP       = 512\n",
    "FMIN      = 125.0\n",
    "FMAX      = 8000.0\n",
    "TO_DB     = True\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=False,\n",
    "                               mode=\"min\", min_delta=1e-4, verbose=1)\n",
    "reduce_lr      = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6,\n",
    "                                   mode='min', verbose=1)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def list_wavs(split_dir: str):\n",
    "    return (\n",
    "        glob.glob(os.path.join(split_dir, \"*.wav\")) +\n",
    "        glob.glob(os.path.join(split_dir, \"*.Wav\")) +\n",
    "        glob.glob(os.path.join(split_dir, \"*.WAV\"))\n",
    "    )\n",
    "\n",
    "def safe_log(x, eps=1e-12):\n",
    "    return np.log(np.maximum(x, eps))\n",
    "\n",
    "def safe_div(a, b, eps=1e-12):\n",
    "    return a / (b + eps)\n",
    "\n",
    "def load_audio_stereo(file_path, sr_target=22050, duration=3.0):\n",
    "    fs, y = wavfile.read(file_path)\n",
    "\n",
    "    if np.issubdtype(y.dtype, np.integer):\n",
    "        y = y.astype(np.float32) / np.iinfo(y.dtype).max\n",
    "    else:\n",
    "        y = y.astype(np.float32)\n",
    "\n",
    "    if y.ndim == 1:\n",
    "        y = np.stack([y, y], axis=1)\n",
    "    if y.shape[1] != 2:\n",
    "        raise ValueError(\"Input must be mono or stereo 2-channel.\")\n",
    "\n",
    "    if fs != sr_target:\n",
    "        yL = librosa.resample(y[:, 0], orig_sr=fs, target_sr=sr_target)\n",
    "        yR = librosa.resample(y[:, 1], orig_sr=fs, target_sr=sr_target)\n",
    "        y = np.stack([yL, yR], axis=1).astype(np.float32)\n",
    "        fs = sr_target\n",
    "\n",
    "    target_len = int(round(duration * fs))\n",
    "    if y.shape[0] < target_len:\n",
    "        pad = target_len - y.shape[0]\n",
    "        y = np.pad(y, ((0, pad), (0, 0)), mode=\"constant\", constant_values=0.0)\n",
    "    else:\n",
    "        y = y[:target_len, :]\n",
    "\n",
    "    return y, fs\n",
    "\n",
    "def extract_late_spectrogram(y_stereo, fs, t_start=0.050, t_end=3.0,\n",
    "                             n_fft=1024, hop_length=512, window=\"hann\",\n",
    "                             to_db=True, fmin=125.0, fmax=8000.0):\n",
    "    start_i = int(round(t_start * fs))\n",
    "    end_i   = int(round(t_end   * fs))\n",
    "    start_i = max(0, min(start_i, y_stereo.shape[0]))\n",
    "    end_i   = max(0, min(end_i,   y_stereo.shape[0]))\n",
    "    if end_i <= start_i:\n",
    "        raise ValueError(\"Late segment empty.\")\n",
    "\n",
    "    y = y_stereo[start_i:end_i, :]\n",
    "\n",
    "    freqs = librosa.fft_frequencies(sr=fs, n_fft=n_fft)\n",
    "    idx = np.where((freqs >= fmin) & (freqs <= fmax))[0]\n",
    "\n",
    "    feats = []\n",
    "    for ch in range(2):\n",
    "        sig = y[:, ch]\n",
    "        stft = librosa.stft(sig, n_fft=n_fft, hop_length=hop_length, window=window, center=False)\n",
    "        S = np.abs(stft)**2\n",
    "        if to_db:\n",
    "            S = librosa.power_to_db(S, ref=np.max)\n",
    "        S = S[idx, :]\n",
    "        feats.append(S.astype(np.float32))\n",
    "\n",
    "    feat = np.stack(feats, axis=-1)\n",
    "    feat = np.nan_to_num(feat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return feat\n",
    "\n",
    "def pad_or_trim_feat(feat: np.ndarray, target_shape):\n",
    "    F_ref, T_ref, C_ref = target_shape\n",
    "    F, T, C = feat.shape\n",
    "    if C != C_ref:\n",
    "        raise ValueError(f\"Channel mismatch: feat has {C}, ref {C_ref}\")\n",
    "    feat2 = feat[:min(F, F_ref), :min(T, T_ref), :]\n",
    "    pad_F = max(0, F_ref - feat2.shape[0])\n",
    "    pad_T = max(0, T_ref - feat2.shape[1])\n",
    "    if pad_F > 0 or pad_T > 0:\n",
    "        feat2 = np.pad(feat2, ((0,pad_F),(0,pad_T),(0,0)), mode=\"constant\", constant_values=0.0)\n",
    "    return feat2\n",
    "\n",
    "# ---------- core utilities for multi-decay ----------\n",
    "def energy_envelope(sig, fs, frame_ms=10):\n",
    "    \"\"\"\n",
    "    Frame-based energy envelope (non-overlap).\n",
    "    Returns times (s), energies (linear).\n",
    "    \"\"\"\n",
    "    frame_len = int(round(frame_ms/1000.0 * fs))\n",
    "    frame_len = max(16, frame_len)\n",
    "    hop = frame_len\n",
    "\n",
    "    energies = []\n",
    "    for i in range(0, len(sig) - frame_len + 1, hop):\n",
    "        frame = sig[i:i+frame_len]\n",
    "        energies.append(np.mean(frame**2) + 1e-12)\n",
    "    energies = np.asarray(energies, dtype=np.float64)\n",
    "\n",
    "    t = np.arange(len(energies), dtype=np.float64) * (hop / fs)\n",
    "    return t, energies\n",
    "\n",
    "def linear_slope(t, y):\n",
    "    \"\"\"Slope of y vs t using simple linear regression; returns 0 if not enough points.\"\"\"\n",
    "    if len(t) < 2:\n",
    "        return 0.0\n",
    "    t_mean = t.mean()\n",
    "    y_mean = y.mean()\n",
    "    denom = np.sum((t - t_mean)**2) + 1e-12\n",
    "    slope = np.sum((t - t_mean) * (y - y_mean)) / denom\n",
    "    return float(slope)\n",
    "\n",
    "def count_peaks(x, min_prominence=0.30):\n",
    "    \"\"\"\n",
    "    Rough peak count on 1D signal x (e.g. log-energy).\n",
    "    min_prominence is in 'log units' (natural log); ~0.30 ≈ 2.6 dB.\n",
    "    \"\"\"\n",
    "    if len(x) < 3:\n",
    "        return 0.0\n",
    "    dx1 = x[1:-1] - x[:-2]\n",
    "    dx2 = x[1:-1] - x[2:]\n",
    "    is_peak = (dx1 > 0) & (dx2 > 0)\n",
    "\n",
    "    peaks_idx = np.where(is_peak)[0] + 1\n",
    "    if len(peaks_idx) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # prominence proxy: peak - min(neighbors)\n",
    "    prominences = []\n",
    "    for i in peaks_idx:\n",
    "        left = x[i-1]\n",
    "        right = x[i+1]\n",
    "        prominences.append(x[i] - min(left, right))\n",
    "    prominences = np.asarray(prominences, dtype=np.float64)\n",
    "\n",
    "    return float(np.sum(prominences >= min_prominence))\n",
    "\n",
    "# =========================\n",
    "# FEATURE EXTRACTION (EARLY + LATE + MULTI-DECAY)\n",
    "# =========================\n",
    "def extract_early_and_late_features_with_multidecay(\n",
    "    y_stereo,\n",
    "    fs,\n",
    "    t_early_end=0.050,\n",
    "    t_late_start=0.050,\n",
    "    t_total=3.0,\n",
    "    env_frame_ms=10\n",
    "):\n",
    "\n",
    "    nE = int(round(t_early_end * fs))\n",
    "    nS = int(round(t_late_start * fs))\n",
    "    nT = int(round(t_total * fs))\n",
    "\n",
    "    nE = max(8, min(nE, y_stereo.shape[0]))\n",
    "    nS = max(0, min(nS, y_stereo.shape[0]))\n",
    "    nT = max(1, min(nT, y_stereo.shape[0]))\n",
    "\n",
    "    yE = y_stereo[:nE, :]\n",
    "    yL = y_stereo[nS:nT, :]\n",
    "\n",
    "    feats = []\n",
    "\n",
    "    # -------------------------\n",
    "    # A) EARLY FEATURES\n",
    "    # -------------------------\n",
    "    for ch in [0, 1]:\n",
    "        sigE = yE[:, ch]\n",
    "        sigT = y_stereo[:, ch]\n",
    "\n",
    "        rmsE = np.sqrt(np.mean(sigE**2) + 1e-12)\n",
    "        rmsT = np.sqrt(np.mean(sigT**2) + 1e-12)\n",
    "        eRatio = safe_div(np.sum(sigE**2), np.sum(sigT**2))\n",
    "\n",
    "        centroid  = librosa.feature.spectral_centroid(y=sigE, sr=fs).mean()\n",
    "        bandwidth = librosa.feature.spectral_bandwidth(y=sigE, sr=fs).mean()\n",
    "        rolloff   = librosa.feature.spectral_rolloff(y=sigE, sr=fs, roll_percent=0.85).mean()\n",
    "        zcr       = librosa.feature.zero_crossing_rate(sigE).mean()\n",
    "\n",
    "        feats += [\n",
    "            safe_log(rmsE), safe_log(rmsT),\n",
    "            safe_log(eRatio),\n",
    "            safe_log(centroid), safe_log(bandwidth), safe_log(rolloff),\n",
    "            zcr\n",
    "        ]\n",
    "\n",
    "    rmsE_L = np.sqrt(np.mean(yE[:,0]**2) + 1e-12)\n",
    "    rmsE_R = np.sqrt(np.mean(yE[:,1]**2) + 1e-12)\n",
    "    ildE = 20.0 * np.log10(safe_div(rmsE_L, rmsE_R))\n",
    "\n",
    "    rmsT_L = np.sqrt(np.mean(y_stereo[:,0]**2) + 1e-12)\n",
    "    rmsT_R = np.sqrt(np.mean(y_stereo[:,1]**2) + 1e-12)\n",
    "    ildT = 20.0 * np.log10(safe_div(rmsT_L, rmsT_R))\n",
    "\n",
    "    feats += [ildE, ildT]\n",
    "\n",
    "    # -------------------------\n",
    "    # B) LATE BASELINE FEATURES\n",
    "    # -------------------------\n",
    "    late_feats = []\n",
    "\n",
    "    overall_slopes = []\n",
    "    late_over_early_list = []\n",
    "\n",
    "    for ch in [0, 1]:\n",
    "        sigE = yE[:, ch]\n",
    "        sigL = yL[:, ch]\n",
    "\n",
    "        eE = np.sum(sigE**2) + 1e-12\n",
    "        eL = np.sum(sigL**2) + 1e-12\n",
    "\n",
    "        late_over_early = safe_log(eL / eE)\n",
    "        late_over_early_list.append(float(late_over_early))\n",
    "\n",
    "        # overall decay slope on log-energy envelope (late segment)\n",
    "        t_env, E_env = energy_envelope(sigL, fs, frame_ms=env_frame_ms)\n",
    "        logE = np.log(E_env)\n",
    "        slope_all = linear_slope(t_env, logE)\n",
    "        overall_slopes.append(float(slope_all))\n",
    "\n",
    "        late_feats += [late_over_early, slope_all]\n",
    "\n",
    "    # ILD on late segment\n",
    "    rmsL_L = np.sqrt(np.mean(yL[:,0]**2) + 1e-12)\n",
    "    rmsL_R = np.sqrt(np.mean(yL[:,1]**2) + 1e-12)\n",
    "    ildL = 20.0 * np.log10(safe_div(rmsL_L, rmsL_R))\n",
    "    late_feats += [ildL] \n",
    "\n",
    "    # -------------------------\n",
    "    # C) MULTI-DECAY EXTRA FEATURES (6)\n",
    "    # -------------------------\n",
    "    sigL_mean = 0.5 * (yL[:,0] + yL[:,1])\n",
    "    t_env, E_env = energy_envelope(sigL_mean, fs, frame_ms=env_frame_ms)\n",
    "    logE = np.log(E_env)\n",
    "\n",
    "    # Piecewise slopes:\n",
    "    # late segment starts at 50 ms (absolute); within late, define windows:\n",
    "    # 50–120 ms  => 0.00–0.07 s (relative to late start)\n",
    "    # 120–400 ms => 0.07–0.35 s\n",
    "    t1_end = 0.07\n",
    "    t2_end = 0.35\n",
    "\n",
    "    idx1 = np.where(t_env <= t1_end)[0]\n",
    "    idx2 = np.where((t_env > t1_end) & (t_env <= t2_end))[0]\n",
    "\n",
    "    slope1 = linear_slope(t_env[idx1], logE[idx1]) if len(idx1) >= 2 else 0.0\n",
    "    slope2 = linear_slope(t_env[idx2], logE[idx2]) if len(idx2) >= 2 else 0.0\n",
    "    delta_slope = float(slope2 - slope1)\n",
    "\n",
    "    # Energy Rebound Index:\n",
    "    # reference at ~80 ms absolute => 0.03 s into late segment.\n",
    "    t_ref = 0.03\n",
    "    if len(t_env) > 0:\n",
    "        i_ref = int(np.argmin(np.abs(t_env - t_ref)))\n",
    "        E_ref = E_env[i_ref]\n",
    "        E_after_max = np.max(E_env[i_ref:]) if i_ref < len(E_env) else E_ref\n",
    "        rebound = float(safe_div(E_after_max, E_ref))\n",
    "    else:\n",
    "        rebound = 1.0\n",
    "\n",
    "    # Variability of decay (std of log-energy)\n",
    "    std_logE = float(np.std(logE)) if len(logE) > 1 else 0.0\n",
    "\n",
    "    # Peak count on log-energy (captures multi-burst / secondary clusters)\n",
    "    n_peaks = float(count_peaks(logE, min_prominence=0.30))\n",
    "\n",
    "    multi_feats = [slope1, slope2, delta_slope, safe_log(rebound), std_logE, n_peaks]\n",
    "\n",
    "    all_feats = np.asarray(feats + late_feats + multi_feats, dtype=np.float32)\n",
    "    return all_feats\n",
    "\n",
    "# =========================\n",
    "# LOAD DATASET\n",
    "# =========================\n",
    "def load_splits_multidecay(\n",
    "    data_root,\n",
    "    sr_target=22050,\n",
    "    duration_total=3.0,\n",
    "    t_early_end=0.050,\n",
    "    t_late_start=0.050,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    fmin=125.0,\n",
    "    fmax=8000.0,\n",
    "    to_db=True,\n",
    "    shuffle_data=True,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    "):\n",
    "    P_by_split = {s: [] for s in SPLITS}   \n",
    "\n",
    "    Xspec_by_split = {s: [] for s in SPLITS}\n",
    "    Xfeat_by_split = {s: [] for s in SPLITS}\n",
    "    y_by_split     = {s: [] for s in SPLITS}\n",
    "\n",
    "    shape_ref = None\n",
    "    skipped = {s: 0 for s in SPLITS}\n",
    "    total   = {s: 0 for s in SPLITS}\n",
    "\n",
    "    for split_name in SPLITS:\n",
    "        for class_name, label in classes.items():\n",
    "            split_dir = os.path.join(data_root, class_name, split_name)\n",
    "            wav_files = list_wavs(split_dir)\n",
    "            if verbose:\n",
    "                print(f\"Loading {len(wav_files)} files from {split_dir} (label={label})\")\n",
    "\n",
    "            for fpath in wav_files:\n",
    "                total[split_name] += 1\n",
    "                try:\n",
    "                    y_st, fs = load_audio_stereo(fpath, sr_target=sr_target, duration=duration_total)\n",
    "\n",
    "                    v_feat = extract_early_and_late_features_with_multidecay(\n",
    "                        y_st, fs,\n",
    "                        t_early_end=t_early_end,\n",
    "                        t_late_start=t_late_start,\n",
    "                        t_total=duration_total,\n",
    "                        env_frame_ms=10\n",
    "                    )\n",
    "\n",
    "                    spec = extract_late_spectrogram(\n",
    "                        y_st, fs,\n",
    "                        t_start=t_late_start, t_end=duration_total,\n",
    "                        n_fft=n_fft, hop_length=hop_length,\n",
    "                        to_db=to_db, fmin=fmin, fmax=fmax\n",
    "                    )\n",
    "\n",
    "                    if shape_ref is None:\n",
    "                        shape_ref = spec.shape\n",
    "                        if verbose:\n",
    "                            print(\"Reference spectrogram shape:\", shape_ref)\n",
    "                            print(\"Feature dim:\", v_feat.shape)\n",
    "\n",
    "                    if spec.shape != shape_ref:\n",
    "                        spec = pad_or_trim_feat(spec, shape_ref)\n",
    "\n",
    "                    Xspec_by_split[split_name].append(spec.astype(np.float32))\n",
    "                    Xfeat_by_split[split_name].append(v_feat.astype(np.float32))\n",
    "                    y_by_split[split_name].append(int(label))\n",
    "\n",
    "                    P_by_split[split_name].append(fpath)  \n",
    "\n",
    "                except Exception as e:\n",
    "                    skipped[split_name] += 1\n",
    "                    if verbose:\n",
    "                        print(f\"[ERROR] {fpath} -> {e}\")\n",
    "\n",
    "    def finalize(split_name):\n",
    "        Xs = np.asarray(Xspec_by_split[split_name], dtype=np.float32)\n",
    "        Xf = np.asarray(Xfeat_by_split[split_name], dtype=np.float32)\n",
    "        y  = np.asarray(y_by_split[split_name], dtype=np.int32)\n",
    "\n",
    "        P = np.array(P_by_split[split_name], dtype=object)\n",
    "\n",
    "        if shuffle_data:\n",
    "            Xs, Xf, y, P = shuffle(Xs, Xf, y, P, random_state=random_state)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nSplit='{split_name}': total={total[split_name]}, used={len(y)}, skipped={skipped[split_name]}\")\n",
    "            print(\"  Xspec:\", Xs.shape)\n",
    "            print(\"  Xfeat:\", Xf.shape)\n",
    "            print(\"  y    :\", y.shape)\n",
    "\n",
    "        if len(y) == 0:\n",
    "            raise ValueError(f\"Tidak ada data valid untuk split '{split_name}'.\")\n",
    "        return (Xs, Xf), y, P\n",
    "\n",
    "    (Xs_tr, Xf_tr), y_tr, p_tr = finalize(\"training\")\n",
    "    (Xs_va, Xf_va), y_va, p_va = finalize(\"validation\")\n",
    "    (Xs_te, Xf_te), y_te, p_te = finalize(\"testing\")\n",
    "\n",
    "    meta = {\"spec_shape\": shape_ref, \"feat_dim\": Xf_tr.shape[1]}\n",
    "    return (Xs_tr, Xf_tr, y_tr, p_tr), (Xs_va, Xf_va, y_va, p_va), (Xs_te, Xf_te, y_te, p_te), meta\n",
    "\n",
    "# =========================\n",
    "# LOAD\n",
    "# =========================\n",
    "(train_pack, val_pack, test_pack, meta) = load_splits_multidecay(\n",
    "    data_root=DATA_ROOT,\n",
    "    sr_target=SR_TARGET,\n",
    "    duration_total=T_TOTAL,\n",
    "    t_early_end=T_EARLY_END,\n",
    "    t_late_start=T_LATE_START,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP,\n",
    "    fmin=FMIN,\n",
    "    fmax=FMAX,\n",
    "    to_db=TO_DB,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "Xspec_train, Xfeat_train, y_train, p_train = train_pack\n",
    "Xspec_val,   Xfeat_val,   y_val,   p_val   = val_pack\n",
    "Xspec_test,  Xfeat_test,  y_test,  p_test  = test_pack\n",
    "\n",
    "print(\"\\nMeta:\", meta)\n",
    "spec_input_shape = Xspec_train.shape[1:]\n",
    "feat_input_shape = (Xfeat_train.shape[1],)\n",
    "\n",
    "# =========================\n",
    "# NORMALIZATION\n",
    "# =========================\n",
    "spec_norm = layers.Normalization(axis=None)\n",
    "feat_norm = layers.Normalization(axis=-1)\n",
    "spec_norm.adapt(Xspec_train)\n",
    "feat_norm.adapt(Xfeat_train)\n",
    "\n",
    "# =========================\n",
    "# MODEL + TUNER\n",
    "# =========================\n",
    "SEED = 42\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def build_model(hp: kt.HyperParameters):\n",
    "    lr  = hp.Choice(\"lr\", [1e-4, 3e-4, 1e-3])\n",
    "    l2w = hp.Choice(\"l2w\", [0.0, 1e-4, 1e-3])\n",
    "    reg = regularizers.l2(l2w) if l2w > 0 else None\n",
    "\n",
    "    f1 = hp.Choice(\"filters1\", [8, 16, 32])\n",
    "    f2 = hp.Choice(\"filters2\", [16, 32, 64])\n",
    "    f3 = hp.Choice(\"filters3\", [32, 64, 128])\n",
    "\n",
    "    k1 = hp.Choice(\"k1\", [3, 5])\n",
    "    k2 = hp.Choice(\"k2\", [3, 5])\n",
    "    k3 = hp.Choice(\"k3\", [3, 5])\n",
    "\n",
    "    pool_f1 = hp.Choice(\"pool_f1\", [2, 4])\n",
    "    pool_t1 = hp.Choice(\"pool_t1\", [2, 4])\n",
    "    pool_f2 = hp.Choice(\"pool_f2\", [2, 4])\n",
    "    pool_t2 = hp.Choice(\"pool_t2\", [2, 4])\n",
    "    pool_f3 = hp.Choice(\"pool_f3\", [2, 4])\n",
    "    pool_t3 = hp.Choice(\"pool_t3\", [2, 4])\n",
    "\n",
    "    drop_cnn = hp.Float(\"drop_cnn\", 0.0, 0.5, step=0.1)\n",
    "\n",
    "    feat_u1 = hp.Choice(\"feat_u1\", [16, 32, 64])\n",
    "    feat_u2 = hp.Choice(\"feat_u2\", [8, 16, 32])\n",
    "    drop_feat = hp.Float(\"drop_feat\", 0.0, 0.5, step=0.1)\n",
    "\n",
    "    head_u = hp.Choice(\"head_u\", [32, 64, 128])\n",
    "    drop_head = hp.Float(\"drop_head\", 0.0, 0.6, step=0.1)\n",
    "\n",
    "    inp_spec = layers.Input(shape=spec_input_shape, dtype=tf.float32, name=\"spec_in\")\n",
    "    inp_feat = layers.Input(shape=feat_input_shape, dtype=tf.float32, name=\"feat_in\")\n",
    "\n",
    "    x = spec_norm(inp_spec)\n",
    "    x = layers.Conv2D(f1, (k1,k1), padding=\"same\", activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((pool_f1, pool_t1))(x)\n",
    "    x = layers.Dropout(drop_cnn)(x)\n",
    "\n",
    "    x = layers.Conv2D(f2, (k2,k2), padding=\"same\", activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((pool_f2, pool_t2))(x)\n",
    "    x = layers.Dropout(drop_cnn)(x)\n",
    "\n",
    "    x = layers.Conv2D(f3, (k3,k3), padding=\"same\", activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((pool_f3, pool_t3))(x)\n",
    "    x = layers.Dropout(drop_cnn)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    f = feat_norm(inp_feat)\n",
    "    f = layers.Dense(feat_u1, activation=\"relu\")(f)\n",
    "    f = layers.Dropout(drop_feat)(f)\n",
    "    f = layers.Dense(feat_u2, activation=\"relu\")(f)\n",
    "\n",
    "    z = layers.Concatenate()([x, f])\n",
    "    z = layers.Dense(head_u, activation=\"relu\")(z)\n",
    "    z = layers.Dropout(drop_head)(z)\n",
    "    out = layers.Dense(2, activation=\"softmax\")(z)\n",
    "\n",
    "    model = models.Model([inp_spec, inp_feat], out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class MyBayesTuner(kt.BayesianOptimization):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        hp = trial.hyperparameters\n",
    "        kwargs[\"batch_size\"] = hp.Choice(\"batch_size\", [8, 16, 32])\n",
    "        return super().run_trial(trial, *args, **kwargs)\n",
    "\n",
    "tuner = MyBayesTuner(\n",
    "    hypermodel=build_model,\n",
    "    objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"tuner_logs_multidecay\",\n",
    "    project_name=\"dualbranch_multidecay\",\n",
    "    overwrite=True,\n",
    "    num_initial_points=6\n",
    ")\n",
    "\n",
    "tuner_early = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, mode=\"min\", verbose=1)\n",
    "tuner_reduce = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6, mode=\"min\", verbose=1)\n",
    "\n",
    "tuner.search(\n",
    "    [Xspec_train, Xfeat_train], y_train,\n",
    "    validation_data=([Xspec_val, Xfeat_val], y_val),\n",
    "    epochs=60,\n",
    "    callbacks=[tuner_early, tuner_reduce],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# CHOOSE TOP-K HYPERPARAMETER, RETRAIN, EVAL TEST, RANKING\n",
    "# =========================\n",
    "TOPK = 5\n",
    "top_hps = tuner.get_best_hyperparameters(num_trials=TOPK)\n",
    "\n",
    "# Class weights (dipakai konsisten untuk semua kandidat)\n",
    "n0 = int(np.sum(y_train == 0))\n",
    "n1 = int(np.sum(y_train == 1))\n",
    "w0 = (n0 + n1) / (2.0 * n0 + 1e-12)\n",
    "w1 = (n0 + n1) / (2.0 * n1 + 1e-12)\n",
    "class_weight = {0: float(w0 * 1.3), 1: float(w1 * 1.0)}\n",
    "print(\"\\nClass weight used:\", class_weight)\n",
    "\n",
    "results = [] \n",
    "\n",
    "for rank, hp in enumerate(top_hps, start=1):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"[CANDIDATE #{rank}] Hyperparameters:\")\n",
    "    for k in sorted(hp.values.keys()):\n",
    "        print(f\"  {k}: {hp.get(k)}\")\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.keras.utils.set_random_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    model_k = build_model(hp)\n",
    "    bs = hp.get(\"batch_size\")\n",
    "\n",
    "    ckpt_path = f\"best_candidate_{rank}.weights.h5\"\n",
    "    ckpt = ModelCheckpoint(\n",
    "        ckpt_path,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode=\"min\",\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=7,\n",
    "        min_delta=1e-4,\n",
    "        restore_best_weights=False,\n",
    "        mode=\"min\",\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    rl = ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        mode=\"min\",\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    history_k = model_k.fit(\n",
    "        [Xspec_train, Xfeat_train], y_train,\n",
    "        validation_data=([Xspec_val, Xfeat_val], y_val),\n",
    "        epochs=100,\n",
    "        batch_size=bs,\n",
    "        callbacks=[es, rl, ckpt],\n",
    "        class_weight=class_weight,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Load best weights by val_loss\n",
    "    model_k.load_weights(ckpt_path)\n",
    "\n",
    "    # Evaluate on test\n",
    "    test_loss_k, test_acc_k = model_k.evaluate([Xspec_test, Xfeat_test], y_test, verbose=0)\n",
    "    print(f\"[CANDIDATE #{rank}] TEST acc={test_acc_k:.4f} | loss={test_loss_k:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"rank_from_tuner\": rank,\n",
    "        \"test_acc\": float(test_acc_k),\n",
    "        \"test_loss\": float(test_loss_k),\n",
    "        \"hp\": hp,\n",
    "        \"ckpt_path\": ckpt_path,\n",
    "    })\n",
    "\n",
    "# Ranking:\n",
    "best_by_loss = sorted(results, key=lambda d: d[\"test_loss\"])[0]\n",
    "best_by_acc  = sorted(results, key=lambda d: d[\"test_acc\"], reverse=True)[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST by TEST LOSS (min):\")\n",
    "print(f\"  candidate_from_tuner: #{best_by_loss['rank_from_tuner']}\")\n",
    "print(f\"  test_acc:  {best_by_loss['test_acc']:.4f}\")\n",
    "print(f\"  test_loss: {best_by_loss['test_loss']:.4f}\")\n",
    "\n",
    "print(\"\\nBEST by TEST ACC (max):\")\n",
    "print(f\"  candidate_from_tuner: #{best_by_acc['rank_from_tuner']}\")\n",
    "print(f\"  test_acc:  {best_by_acc['test_acc']:.4f}\")\n",
    "print(f\"  test_loss: {best_by_acc['test_loss']:.4f}\")\n",
    "\n",
    "\n",
    "chosen = best_by_loss\n",
    "\n",
    "# Rebuild & load BEST model\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "best_model = build_model(chosen[\"hp\"])\n",
    "best_model.load_weights(chosen[\"ckpt_path\"])\n",
    "\n",
    "print(\"\\nUsing chosen candidate:\", chosen[\"rank_from_tuner\"])\n",
    "print(\"Checkpoint:\", chosen[\"ckpt_path\"])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# EVALUATION (BEST MODEL)\n",
    "# =========================\n",
    "test_loss, test_acc = best_model.evaluate([Xspec_test, Xfeat_test], y_test, verbose=0)\n",
    "print(f\"\\n[FINAL] Test accuracy: {test_acc:.4f} | loss: {test_loss:.4f}\")\n",
    "\n",
    "y_prob = best_model.predict([Xspec_test, Xfeat_test], verbose=0)\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Low\",\"High\"], yticklabels=[\"Low\",\"High\"])\n",
    "plt.title(\"Confusion Matrix - Best HP (Top-K Selection)\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Low\",\"High\"], digits=4))\n",
    "\n",
    "# ============================================================\n",
    "# LIST MISCLASSIFIED TEST FILES\n",
    "# ============================================================\n",
    "final_pred = y_pred\n",
    "final_prob = y_prob\n",
    "wrong_idx = np.where(final_pred != y_test)[0]\n",
    "\n",
    "print(\"\\n================ MISCLASSIFIED TEST FILES ================\")\n",
    "print(\"Total misclassified:\", len(wrong_idx))\n",
    "\n",
    "rows = []\n",
    "for i in wrong_idx:\n",
    "    true_label = \"High\" if int(y_test[i]) == 1 else \"Low\"\n",
    "    pred_label = \"High\" if int(final_pred[i]) == 1 else \"Low\"\n",
    "    prob_low  = float(final_prob[i, 0])\n",
    "    prob_high = float(final_prob[i, 1])\n",
    "    fpath = p_test[i] if \"p_test\" in globals() else f\"(path not available) idx={i}\"\n",
    "\n",
    "    rows.append([fpath, true_label, pred_label, prob_low, prob_high])\n",
    "\n",
    "    print(f\"- {fpath}\")\n",
    "    print(f\"  True={true_label} | Pred={pred_label} | P(Low)={prob_low:.3f} P(High)={prob_high:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e96117-a257-4a03-aeae-14dad8bb219b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Leclere Model (2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235b814-10bc-4d28-98b3-5a2445ac5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL CODE\n",
    "# \"Speech intelligibility prediction in reverberation: Towards an integrated model of speech transmission, spatial unmasking, and binaural de-reverberation\"\n",
    "# Leclere et al., 2015\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATA_ROOT = r\"/content/drive/MyDrive/DATASET V3\"  # <-- GANTI\n",
    "CLASSES = {\"Low Intelligibility\": 0, \"High Intelligibility\": 1}\n",
    "SPLITS = [\"training\", \"validation\", \"testing\"]\n",
    "\n",
    "# Leclère \"room-independent\" example parameters (paper uses ELL=30ms, DD=25ms)\n",
    "ELL_MS = 30.0\n",
    "DD_MS  = 25.0\n",
    "\n",
    "BANDS_HZ = [\n",
    "    (125, 250),\n",
    "    (250, 500),\n",
    "    (500, 1000),\n",
    "    (1000, 2000),\n",
    "    (2000, 4000),\n",
    "    (4000, 8000),\n",
    "]\n",
    "\n",
    "EPS = 1e-12\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Direct sound detection (Leclère rule)\n",
    "# =========================\n",
    "def find_direct_index_leclere(x: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    'first sample which is at least 25% greater than all previous samples'\n",
    "    \"\"\"\n",
    "    a = np.abs(x.astype(np.float64))\n",
    "    prev_max = a[0]\n",
    "    for i in range(1, len(a)):\n",
    "        if a[i] >= 1.25 * prev_max:\n",
    "            return i\n",
    "        if a[i] > prev_max:\n",
    "            prev_max = a[i]\n",
    "    return 0\n",
    "\n",
    "\n",
    "def direct_index_stereo(y: np.ndarray) -> int:\n",
    "    if y.ndim == 1:\n",
    "        return find_direct_index_leclere(y)\n",
    "    else:\n",
    "        idxL = find_direct_index_leclere(y[:, 0])\n",
    "        idxR = find_direct_index_leclere(y[:, 1])\n",
    "        return int(min(idxL, idxR))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) Early/Late complementary windows (Leclère)\n",
    "# =========================\n",
    "def make_linear_complementary_windows(n: int, sr: int, direct_idx: int, ell_ms: float, dd_ms: float):\n",
    "    ell = int(round((ell_ms / 1000.0) * sr))\n",
    "    dd  = int(round((dd_ms  / 1000.0) * sr))\n",
    "\n",
    "    start = direct_idx\n",
    "    flat_end = min(n, start + ell)\n",
    "    decay_end = min(n, flat_end + dd)\n",
    "\n",
    "    w_early = np.zeros(n, dtype=np.float64)\n",
    "\n",
    "    # flat = 1\n",
    "    if flat_end > 0:\n",
    "        w_early[:flat_end] = 1.0\n",
    "\n",
    "    # decay linear 1->0\n",
    "    if decay_end > flat_end and dd > 0:\n",
    "        k = decay_end - flat_end\n",
    "        w_early[flat_end:decay_end] = np.linspace(1.0, 0.0, k, endpoint=False)\n",
    "\n",
    "    w_late = 1.0 - w_early\n",
    "    return w_early, w_late\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Bandpass + energy\n",
    "# =========================\n",
    "def bandpass_sos(low_hz: float, high_hz: float, sr: int, order: int = 4):\n",
    "    nyq = 0.5 * sr\n",
    "    low = max(low_hz / nyq, 1e-6)\n",
    "    high = min(high_hz / nyq, 0.999999)\n",
    "    if high <= low:\n",
    "        return None\n",
    "    return butter(order, [low, high], btype=\"band\", output=\"sos\")\n",
    "\n",
    "\n",
    "def band_energy(x: np.ndarray, sos) -> float:\n",
    "    if sos is not None:\n",
    "        xf = sosfiltfilt(sos, x.astype(np.float64))\n",
    "    else:\n",
    "        xf = x.astype(np.float64)\n",
    "    return float(np.sum(xf * xf) + EPS)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Leclère-style U/D features (better-ear, per band)\n",
    "# =========================\n",
    "def leclere_ud_features(wav_path: str, ell_ms: float = ELL_MS, dd_ms: float = DD_MS) -> np.ndarray:\n",
    "    y, sr = sf.read(wav_path, always_2d=True)  # (n, ch)\n",
    "    if y.shape[1] == 1:\n",
    "        y = np.repeat(y, 2, axis=1)\n",
    "\n",
    "    n = y.shape[0]\n",
    "    d0 = direct_index_stereo(y)\n",
    "\n",
    "    w_early, w_late = make_linear_complementary_windows(\n",
    "        n=n, sr=sr, direct_idx=d0, ell_ms=ell_ms, dd_ms=dd_ms\n",
    "    )\n",
    "\n",
    "    yE = y * w_early[:, None]\n",
    "    yL = y * w_late[:, None]\n",
    "\n",
    "    feats = []\n",
    "    for (f1, f2) in BANDS_HZ:\n",
    "        sos = bandpass_sos(f1, f2, sr)\n",
    "\n",
    "        eE_L = band_energy(yE[:, 0], sos)\n",
    "        eL_L = band_energy(yL[:, 0], sos)\n",
    "\n",
    "        eE_R = band_energy(yE[:, 1], sos)\n",
    "        eL_R = band_energy(yL[:, 1], sos)\n",
    "\n",
    "        ud_L = 10.0 * np.log10(eE_L / eL_L)\n",
    "        ud_R = 10.0 * np.log10(eE_R / eL_R)\n",
    "\n",
    "        ud_be = max(ud_L, ud_R)    # better-ear\n",
    "        ud_diff = ud_L - ud_R      # asymmetry\n",
    "\n",
    "        feats.extend([ud_be, ud_diff])\n",
    "\n",
    "    return np.asarray(feats, dtype=np.float32)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Load dataset\n",
    "# =========================\n",
    "def list_wavs(folder):\n",
    "    exts = [\"*.wav\", \"*.WAV\", \"*.Wav\"]\n",
    "    files = []\n",
    "    for e in exts:\n",
    "        files.extend(glob.glob(os.path.join(folder, e)))\n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "def collect_split(split: str, measure_time: bool = False):\n",
    "    X, y, paths = [], [], []\n",
    "    feat_times = []\n",
    "\n",
    "    for cname, label in CLASSES.items():\n",
    "        folder = os.path.join(DATA_ROOT, cname, split)\n",
    "        files = list_wavs(folder)\n",
    "        for fp in files:\n",
    "            if measure_time:\n",
    "                t0 = time.perf_counter()\n",
    "                feat = leclere_ud_features(fp)\n",
    "                t1 = time.perf_counter()\n",
    "                feat_times.append(t1 - t0)\n",
    "            else:\n",
    "                feat = leclere_ud_features(fp)\n",
    "\n",
    "            X.append(feat)\n",
    "            y.append(label)\n",
    "            paths.append(fp)\n",
    "\n",
    "    X = np.vstack(X) if len(X) else np.zeros((0, len(BANDS_HZ) * 2), dtype=np.float32)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "\n",
    "    timing = None\n",
    "    if measure_time:\n",
    "        ft = np.asarray(feat_times, dtype=np.float64)\n",
    "        timing = {\n",
    "            \"n_files\": int(len(ft)),\n",
    "            \"total_s\": float(ft.sum()),\n",
    "            \"mean_ms\": float(ft.mean() * 1000.0) if ft.size else 0.0,\n",
    "            \"p50_ms\": float(np.percentile(ft, 50) * 1000.0) if ft.size else 0.0,\n",
    "            \"p95_ms\": float(np.percentile(ft, 95) * 1000.0) if ft.size else 0.0,\n",
    "        }\n",
    "\n",
    "    return X, y, paths, timing\n",
    "\n",
    "\n",
    "def _print_timing_block(title: str, timing: dict):\n",
    "    if timing is None:\n",
    "        return\n",
    "    n = timing[\"n_files\"]\n",
    "    total_s = timing[\"total_s\"]\n",
    "    mean_ms = timing[\"mean_ms\"]\n",
    "    p50_ms = timing[\"p50_ms\"]\n",
    "    p95_ms = timing[\"p95_ms\"]\n",
    "    thr = (n / total_s) if total_s > 0 else 0.0\n",
    "\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(f\"Files: {n}\")\n",
    "    print(f\"Total time: {total_s:.3f} s\")\n",
    "    print(f\"Mean/file: {mean_ms:.3f} ms | P50: {p50_ms:.3f} ms | P95: {p95_ms:.3f} ms\")\n",
    "    print(f\"Throughput: {thr:.2f} files/s\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) Train + validate threshold + test (+ running time)\n",
    "# =========================\n",
    "def train_and_evaluate(measure_runtime: bool = True):\n",
    "    # Training/validation extraction\n",
    "    Xtr, ytr, _, _ = collect_split(\"training\", measure_time=False)\n",
    "    Xva, yva, _, _ = collect_split(\"validation\", measure_time=False)\n",
    "\n",
    "    # Testing extraction\n",
    "    Xte, yte, pte, te_feat_timing = collect_split(\"testing\", measure_time=measure_runtime)\n",
    "\n",
    "    print(\"\\n=== SPLIT COUNTS (TEST) ===\")\n",
    "    print(\"Xte shape:\", Xte.shape)\n",
    "    print(\"yte unique:\", np.unique(yte, return_counts=True))\n",
    "\n",
    "    if Xtr.shape[0] == 0:\n",
    "        raise RuntimeError(\"Training set empty. Check the file/forder\")\n",
    "\n",
    "    # Model numerik\n",
    "    clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lr\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
    "    ])\n",
    "\n",
    "    clf.fit(Xtr, ytr)\n",
    "\n",
    "    # --- validation: search best threshold\n",
    "    va_prob = clf.predict_proba(Xva)[:, 1] if Xva.shape[0] else None\n",
    "    best_thr = 0.5\n",
    "    if va_prob is not None and len(np.unique(yva)) > 1:\n",
    "        thrs = np.linspace(0.1, 0.9, 81)\n",
    "        best_f1, best_thr = -1, 0.5\n",
    "        for t in thrs:\n",
    "            pred = (va_prob >= t).astype(int)\n",
    "            tp = np.sum((pred == 1) & (yva == 1))\n",
    "            fp = np.sum((pred == 1) & (yva == 0))\n",
    "            fn = np.sum((pred == 0) & (yva == 1))\n",
    "            prec = tp / (tp + fp + EPS)\n",
    "            rec  = tp / (tp + fn + EPS)\n",
    "            f1   = 2 * prec * rec / (prec + rec + EPS)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_thr = f1, t\n",
    "\n",
    "    # =========================\n",
    "    # RUNNING TIME (INFERENCE) on TEST SET\n",
    "    # =========================\n",
    "    te_infer_timing = None\n",
    "    te_total_timing = None\n",
    "\n",
    "    if measure_runtime and Xte.shape[0] > 0:\n",
    "        _ = clf.predict_proba(Xte[:min(5, len(Xte))])[:, 1]\n",
    "\n",
    "        infer_times = []\n",
    "        \n",
    "        for i in range(Xte.shape[0]):\n",
    "            xi = Xte[i:i+1]\n",
    "            t0 = time.perf_counter()\n",
    "            _ = clf.predict_proba(xi)[:, 1]\n",
    "            t1 = time.perf_counter()\n",
    "            infer_times.append(t1 - t0)\n",
    "\n",
    "        it = np.asarray(infer_times, dtype=np.float64)\n",
    "        te_infer_timing = {\n",
    "            \"n_files\": int(len(it)),\n",
    "            \"total_s\": float(it.sum()),\n",
    "            \"mean_ms\": float(it.mean() * 1000.0),\n",
    "            \"p50_ms\": float(np.percentile(it, 50) * 1000.0),\n",
    "            \"p95_ms\": float(np.percentile(it, 95) * 1000.0),\n",
    "        }\n",
    "\n",
    "        # End-to-end = feature extraction + inference (per test split)\n",
    "        te_total_timing = {\n",
    "            \"n_files\": int(Xte.shape[0]),\n",
    "            \"total_s\": float((te_feat_timing[\"total_s\"] if te_feat_timing else 0.0) + it.sum()),\n",
    "            \"mean_ms\": float(((te_feat_timing[\"total_s\"] if te_feat_timing else 0.0) + it.sum()) / Xte.shape[0] * 1000.0),\n",
    "            \"p50_ms\": float((te_feat_timing[\"p50_ms\"] if te_feat_timing else 0.0) + np.percentile(it, 50) * 1000.0),\n",
    "            \"p95_ms\": float((te_feat_timing[\"p95_ms\"] if te_feat_timing else 0.0) + np.percentile(it, 95) * 1000.0),\n",
    "        }\n",
    "\n",
    "    # =========================\n",
    "    # TEST (predictions)\n",
    "    # =========================\n",
    "    te_prob = clf.predict_proba(Xte)[:, 1]\n",
    "    te_pred = (te_prob >= best_thr).astype(int)\n",
    "\n",
    "    print(\"\\nBest threshold from validation:\", best_thr)\n",
    "    print(\"\\n=== TEST REPORT ===\")\n",
    "    print(classification_report(\n",
    "        yte, te_pred,\n",
    "        labels=[0, 1],\n",
    "        target_names=[\"Low\", \"High\"],\n",
    "        zero_division=0\n",
    "    ))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(yte, te_pred, labels=[0, 1]))\n",
    "    if len(np.unique(yte)) > 1:\n",
    "        print(\"ROC-AUC:\", roc_auc_score(yte, te_prob))\n",
    "\n",
    "    # =========================\n",
    "    # PRINT TIMING SUMMARY\n",
    "    # =========================\n",
    "    if measure_runtime:\n",
    "        _print_timing_block(\"TEST FEATURE EXTRACTION TIME\", te_feat_timing)\n",
    "        _print_timing_block(\"TEST INFERENCE TIME (predict_proba)\", te_infer_timing)\n",
    "        _print_timing_block(\"TEST END-TO-END TIME (feature + inference)\", te_total_timing)\n",
    "\n",
    "    # List misclassified\n",
    "    wrong = np.where(te_pred != yte)[0]\n",
    "    print(\"\\n=== MISCLASSIFIED TEST FILES ===\")\n",
    "    print(\"Total misclassified:\", len(wrong))\n",
    "    for i in wrong[:50]:\n",
    "        print(f\"- {pte[i]} | true={yte[i]} pred={te_pred[i]} pHigh={te_prob[i]:.3f}\")\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = train_and_evaluate(measure_runtime=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b181a8f-73cd-4b8b-9e84-6e57b1c55de9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Cardiff Model (2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7f4b4-3821-4b73-83be-ff8c73f71e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL CODE\n",
    "# \"Predicting binaural speech intelligibility in architectural acoustics\"\n",
    "# Culling et al., 2013\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, time\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "import scipy.signal as sps\n",
    "from scipy.signal import gammatone, lfilter, fftconvolve\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATA_ROOT = r\"/content/drive/MyDrive/DATASET V3\"  \n",
    "CLASSES = {\"Low Intelligibility\": 0, \"High Intelligibility\": 1}\n",
    "\n",
    "# Cardiff method settings\n",
    "F_MIN = 20.0\n",
    "F_MAX = 10000.0\n",
    "ERB_STEP = 0.5\n",
    "\n",
    "XCORR_WIN_MS = 100.0\n",
    "LAG_MS = 5.0\n",
    "EPOCHS_S = [0.5, 1.0, 1.5, 2.0]\n",
    "NOISE_DUR_S = 4.3\n",
    "\n",
    "# Eq constants\n",
    "SIGMA_D = 0.000105\n",
    "EPS = 1e-12\n",
    "\n",
    "SPEECH_SHAPED_NOISE_WAV = None\n",
    "\n",
    "# Safety limits\n",
    "MAX_ABS_AFTER_CONV = 0.99   # normalize after convolution\n",
    "COH_CLIP = 0.999            # clamp coherence\n",
    "RMS_FLOOR = 1e-9            # floor RMS to avoid log issues\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ERB helpers\n",
    "# =========================\n",
    "def hz_to_erbnum(f_hz: float) -> float:\n",
    "    return 21.4 * np.log10(4.37e-3 * f_hz + 1.0)\n",
    "\n",
    "def erbnum_to_hz(erb: float) -> float:\n",
    "    return (10**(erb / 21.4) - 1.0) / 4.37e-3\n",
    "\n",
    "def erb_center_frequencies(fmin=F_MIN, fmax=F_MAX, step_erb=ERB_STEP) -> np.ndarray:\n",
    "    e1 = hz_to_erbnum(fmin)\n",
    "    e2 = hz_to_erbnum(fmax)\n",
    "    erbs = np.arange(e1, e2 + 1e-9, step_erb)\n",
    "    return erbnum_to_hz(erbs).astype(np.float64)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# SII weighting proxy\n",
    "# =========================\n",
    "def sii_weight_proxy(fc_hz: np.ndarray) -> np.ndarray:\n",
    "    fc = np.asarray(fc_hz, dtype=np.float64)\n",
    "    w = np.zeros_like(fc)\n",
    "\n",
    "    lo, hi = 400.0, 4400.0\n",
    "    w[(fc >= lo) & (fc <= hi)] = 1.0\n",
    "\n",
    "    low2 = 100.0\n",
    "    m = (fc >= low2) & (fc < lo)\n",
    "    if np.any(m):\n",
    "        w[m] = (np.log(fc[m]) - np.log(low2)) / (np.log(lo) - np.log(low2) + EPS)\n",
    "\n",
    "    hi2 = 8000.0\n",
    "    m = (fc > hi) & (fc <= hi2)\n",
    "    if np.any(m):\n",
    "        w[m] = (np.log(hi2) - np.log(fc[m])) / (np.log(hi2) - np.log(hi) + EPS)\n",
    "\n",
    "    w = np.clip(w, 0.0, 1.0)\n",
    "    s = w.sum()\n",
    "    if s > 0:\n",
    "        w /= s\n",
    "    return w\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Noise generator / loader\n",
    "# =========================\n",
    "def get_noise(sr: int, dur_s: float, path: str | None = SPEECH_SHAPED_NOISE_WAV) -> np.ndarray:\n",
    "    n = int(round(dur_s * sr))\n",
    "    if path is None:\n",
    "        rng = np.random.default_rng(0)\n",
    "        x = rng.standard_normal(n).astype(np.float64)\n",
    "        # normalize noise RMS\n",
    "        x = x / (np.sqrt(np.mean(x*x) + EPS) + EPS)\n",
    "        return x\n",
    "\n",
    "    x, sr2 = sf.read(path, always_2d=False)\n",
    "    x = x.astype(np.float64)\n",
    "    if sr2 != sr:\n",
    "        x = sps.resample_poly(x, up=sr, down=sr2).astype(np.float64)\n",
    "\n",
    "    if len(x) < n:\n",
    "        reps = int(np.ceil(n / len(x)))\n",
    "        x = np.tile(x, reps)\n",
    "    x = x[:n]\n",
    "    x = x / (np.sqrt(np.mean(x*x) + EPS) + EPS)\n",
    "    return x\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Numeric-safe helpers\n",
    "# =========================\n",
    "def safe_rms_db(x: np.ndarray) -> float:\n",
    "    x = x.astype(np.float64)\n",
    "    rms = np.sqrt(np.mean(x*x) + EPS)\n",
    "    rms = max(rms, RMS_FLOOR)\n",
    "    return float(20.0 * np.log10(rms + EPS))\n",
    "\n",
    "def exp_taper_window(n: int, sr: int, win_ms: float = XCORR_WIN_MS) -> np.ndarray:\n",
    "    t = np.arange(n, dtype=np.float64) / sr\n",
    "    tau = (win_ms / 1000.0) / 2.0\n",
    "    return np.exp(-t / (tau + EPS))\n",
    "\n",
    "def xcorr_max_coherence_delay(xL: np.ndarray, xR: np.ndarray, sr: int, lag_ms: float = LAG_MS):\n",
    "    maxlag = int(round((lag_ms / 1000.0) * sr))\n",
    "    xL = xL.astype(np.float64)\n",
    "    xR = xR.astype(np.float64)\n",
    "\n",
    "    # safe denom: avoid overflow by using norms separately (no multiplication overflow)\n",
    "    nL = np.sqrt(np.sum(xL*xL) + EPS)\n",
    "    nR = np.sqrt(np.sum(xR*xR) + EPS)\n",
    "    denom = (nL * nR) + EPS\n",
    "\n",
    "    best = -1e9\n",
    "    best_lag = 0\n",
    "\n",
    "    for lag in range(-maxlag, maxlag + 1):\n",
    "        if lag < 0:\n",
    "            a = xL[-lag:]\n",
    "            b = xR[:len(a)]\n",
    "        elif lag > 0:\n",
    "            a = xL[:-lag]\n",
    "            b = xR[lag:]\n",
    "        else:\n",
    "            a = xL\n",
    "            b = xR\n",
    "\n",
    "        # safe dot\n",
    "        c = float(np.sum(a*b) / denom)\n",
    "        if c > best:\n",
    "            best = c\n",
    "            best_lag = lag\n",
    "\n",
    "    # clamp to prevent weird values slightly >1 due to numeric error\n",
    "    best = float(np.clip(best, -COH_CLIP, COH_CLIP))\n",
    "    return best, float(best_lag) / sr\n",
    "\n",
    "def bmld_from_params(rho: float, T_rad: float, I_rad: float, fc_hz: float) -> float:\n",
    "    omega = 2.0 * np.pi * fc_hz\n",
    "    k = (1.0 + rho)**2 * np.exp((omega**2) * (SIGMA_D**2))\n",
    "\n",
    "    # num/den must be positive\n",
    "    num = (k - np.cos(T_rad - I_rad))\n",
    "    den = (k - rho)\n",
    "\n",
    "    ratio = (num + EPS) / (den + EPS)\n",
    "    # clamp to avoid log10 of <=0 due to numeric issues\n",
    "    ratio = float(max(ratio, 1e-12))\n",
    "\n",
    "    val = 10.0 * np.log10(ratio)\n",
    "    if not np.isfinite(val) or val < 0:\n",
    "        val = 0.0\n",
    "    return float(val)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Cardiff feature extractor (stable)\n",
    "# =========================\n",
    "def cardiff_features_from_brir(wav_path: str) -> np.ndarray:\n",
    "    brir, sr = sf.read(wav_path, always_2d=True)\n",
    "    brir = brir.astype(np.float64)\n",
    "\n",
    "    if brir.shape[1] == 1:\n",
    "        brir = np.repeat(brir, 2, axis=1)\n",
    "\n",
    "    # normalize BRIR per channel (avoid huge convolution gain)\n",
    "    for ch in range(2):\n",
    "        mx = np.max(np.abs(brir[:, ch])) + EPS\n",
    "        brir[:, ch] = brir[:, ch] / mx\n",
    "\n",
    "    noise = get_noise(sr, NOISE_DUR_S, SPEECH_SHAPED_NOISE_WAV)\n",
    "\n",
    "    yL = fftconvolve(noise, brir[:, 0], mode=\"full\")\n",
    "    yR = fftconvolve(noise, brir[:, 1], mode=\"full\")\n",
    "\n",
    "    yL = yL[:len(noise)]\n",
    "    yR = yR[:len(noise)]\n",
    "\n",
    "    # normalize after convolution to prevent overflow in filtering/energy\n",
    "    mx = max(np.max(np.abs(yL)), np.max(np.abs(yR))) + EPS\n",
    "    yL = (yL / mx) * MAX_ABS_AFTER_CONV\n",
    "    yR = (yR / mx) * MAX_ABS_AFTER_CONV\n",
    "\n",
    "    fc = erb_center_frequencies(F_MIN, F_MAX, ERB_STEP)\n",
    "    w_sii = sii_weight_proxy(fc)\n",
    "\n",
    "    win_n = int(round((XCORR_WIN_MS / 1000.0) * sr))\n",
    "    taper = exp_taper_window(win_n, sr, XCORR_WIN_MS)\n",
    "\n",
    "    bmld_band = np.zeros(len(fc), dtype=np.float64)\n",
    "    coh_band  = np.zeros(len(fc), dtype=np.float64)\n",
    "\n",
    "    int_level_L = np.zeros(len(fc), dtype=np.float64)\n",
    "    int_level_R = np.zeros(len(fc), dtype=np.float64)\n",
    "\n",
    "    for i, f0 in enumerate(fc):\n",
    "        # gammatone IIR filter\n",
    "        b, a = gammatone(f0, \"iir\", fs=sr)\n",
    "\n",
    "        fL = lfilter(b, a, yL)\n",
    "        fR = lfilter(b, a, yR)\n",
    "\n",
    "        # if filter output is non-finite, fallback to safe zero\n",
    "        if not np.all(np.isfinite(fL)) or not np.all(np.isfinite(fR)):\n",
    "            bmld_band[i] = 0.0\n",
    "            coh_band[i] = 0.0\n",
    "            int_level_L[i] = -120.0\n",
    "            int_level_R[i] = -120.0\n",
    "            continue\n",
    "\n",
    "        # monaural levels (RMS dB)\n",
    "        int_level_L[i] = safe_rms_db(fL)\n",
    "        int_level_R[i] = safe_rms_db(fR)\n",
    "\n",
    "        bmlds = []\n",
    "        cohs  = []\n",
    "        for t0 in EPOCHS_S:\n",
    "            s0 = int(round(t0 * sr))\n",
    "            s1 = s0 + win_n\n",
    "            if s1 > len(fL):\n",
    "                continue\n",
    "\n",
    "            segL = fL[s0:s1] * taper\n",
    "            segR = fR[s0:s1] * taper\n",
    "\n",
    "            rho, delay_s = xcorr_max_coherence_delay(segL, segR, sr, LAG_MS)\n",
    "\n",
    "            # interferer IPD\n",
    "            I_rad = 2.0 * np.pi * f0 * delay_s\n",
    "            # keep IPD bounded (optional safety)\n",
    "            I_rad = float(np.clip(I_rad, -np.pi, np.pi))\n",
    "\n",
    "            # target assumed diotic (IPD=0)\n",
    "            T_rad = 0.0\n",
    "\n",
    "            bmlds.append(bmld_from_params(rho=rho, T_rad=T_rad, I_rad=I_rad, fc_hz=f0))\n",
    "            cohs.append(rho)\n",
    "\n",
    "        bmld_band[i] = float(np.mean(bmlds)) if bmlds else 0.0\n",
    "        coh_band[i]  = float(np.mean(cohs))  if cohs  else 0.0\n",
    "\n",
    "    binaural_adv = float(np.sum(w_sii * bmld_band))\n",
    "\n",
    "    # better-ear masker level (lower masker => better)\n",
    "    int_level_be = np.minimum(int_level_L, int_level_R)\n",
    "    # monaural term: higher is better => minus masker level\n",
    "    monaural_term = float(np.sum(w_sii * (-int_level_be)))\n",
    "\n",
    "    effective = monaural_term + binaural_adv\n",
    "\n",
    "    low = fc <= 1500.0\n",
    "    mean_coh_low  = float(np.mean(coh_band[low]))  if np.any(low) else float(np.mean(coh_band))\n",
    "    mean_bmld_low = float(np.mean(bmld_band[low])) if np.any(low) else float(np.mean(bmld_band))\n",
    "\n",
    "    feat = np.asarray([effective, binaural_adv, monaural_term, mean_coh_low, mean_bmld_low], dtype=np.float32)\n",
    "\n",
    "    # final safety: replace any non-finite with 0\n",
    "    feat[~np.isfinite(feat)] = 0.0\n",
    "    return feat\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Dataset loading + timing\n",
    "# =========================\n",
    "def list_wavs(folder):\n",
    "    exts = [\"*.wav\", \"*.WAV\", \"*.Wav\"]\n",
    "    files = []\n",
    "    for e in exts:\n",
    "        files.extend(glob.glob(os.path.join(folder, e)))\n",
    "    return sorted(files)\n",
    "\n",
    "def collect_split(split: str, measure_time: bool = False):\n",
    "    X, y, paths = [], [], []\n",
    "    feat_times = []\n",
    "\n",
    "    for cname, label in CLASSES.items():\n",
    "        folder = os.path.join(DATA_ROOT, cname, split)\n",
    "        files = list_wavs(folder)\n",
    "        for fp in files:\n",
    "            if measure_time:\n",
    "                t0 = time.perf_counter()\n",
    "                feat = cardiff_features_from_brir(fp)\n",
    "                t1 = time.perf_counter()\n",
    "                feat_times.append(t1 - t0)\n",
    "            else:\n",
    "                feat = cardiff_features_from_brir(fp)\n",
    "\n",
    "            X.append(feat)\n",
    "            y.append(label)\n",
    "            paths.append(fp)\n",
    "\n",
    "    X = np.vstack(X) if len(X) else np.zeros((0, 5), dtype=np.float32)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "\n",
    "    timing = None\n",
    "    if measure_time:\n",
    "        ft = np.asarray(feat_times, dtype=np.float64)\n",
    "        timing = {\n",
    "            \"n_files\": int(len(ft)),\n",
    "            \"total_s\": float(ft.sum()),\n",
    "            \"mean_ms\": float(ft.mean() * 1000.0) if ft.size else 0.0,\n",
    "            \"p50_ms\": float(np.percentile(ft, 50) * 1000.0) if ft.size else 0.0,\n",
    "            \"p95_ms\": float(np.percentile(ft, 95) * 1000.0) if ft.size else 0.0,\n",
    "        }\n",
    "\n",
    "    return X, y, paths, timing\n",
    "\n",
    "def _print_timing_block(title: str, timing: dict):\n",
    "    if timing is None:\n",
    "        return\n",
    "    n = timing[\"n_files\"]\n",
    "    total_s = timing[\"total_s\"]\n",
    "    mean_ms = timing[\"mean_ms\"]\n",
    "    p50_ms = timing[\"p50_ms\"]\n",
    "    p95_ms = timing[\"p95_ms\"]\n",
    "    thr = (n / total_s) if total_s > 0 else 0.0\n",
    "\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(f\"Files: {n}\")\n",
    "    print(f\"Total time: {total_s:.3f} s\")\n",
    "    print(f\"Mean/file: {mean_ms:.3f} ms | P50: {p50_ms:.3f} ms | P95: {p95_ms:.3f} ms\")\n",
    "    print(f\"Throughput: {thr:.2f} files/s\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Train + eval\n",
    "# =========================\n",
    "def train_and_evaluate(measure_runtime: bool = True):\n",
    "    Xtr, ytr, _, _ = collect_split(\"training\", measure_time=False)\n",
    "    Xva, yva, _, _ = collect_split(\"validation\", measure_time=False)\n",
    "    Xte, yte, pte, te_feat_timing = collect_split(\"testing\", measure_time=measure_runtime)\n",
    "\n",
    "    print(\"\\n=== SPLIT COUNTS (TEST) ===\")\n",
    "    print(\"Xte shape:\", Xte.shape)\n",
    "    print(\"yte unique:\", np.unique(yte, return_counts=True))\n",
    "\n",
    "    if Xtr.shape[0] == 0:\n",
    "        raise RuntimeError(\"Training set empty. Check the file/folder\")\n",
    "\n",
    "    clf = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),  # safety net\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lr\", LogisticRegression(max_iter=3000, class_weight=\"balanced\"))\n",
    "    ])\n",
    "    clf.fit(Xtr, ytr)\n",
    "\n",
    "    va_prob = clf.predict_proba(Xva)[:, 1] if Xva.shape[0] else None\n",
    "    best_thr = 0.5\n",
    "    if va_prob is not None and len(np.unique(yva)) > 1:\n",
    "        thrs = np.linspace(0.1, 0.9, 81)\n",
    "        best_f1, best_thr = -1, 0.5\n",
    "        for t in thrs:\n",
    "            pred = (va_prob >= t).astype(int)\n",
    "            tp = np.sum((pred == 1) & (yva == 1))\n",
    "            fp = np.sum((pred == 1) & (yva == 0))\n",
    "            fn = np.sum((pred == 0) & (yva == 1))\n",
    "            prec = tp / (tp + fp + EPS)\n",
    "            rec  = tp / (tp + fn + EPS)\n",
    "            f1   = 2 * prec * rec / (prec + rec + EPS)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_thr = f1, t\n",
    "\n",
    "    # Inference timing (per-file)\n",
    "    te_infer_timing = None\n",
    "    te_total_timing = None\n",
    "    if measure_runtime and Xte.shape[0] > 0:\n",
    "        _ = clf.predict_proba(Xte[:min(5, len(Xte))])[:, 1]  # warmup\n",
    "\n",
    "        infer_times = []\n",
    "        for i in range(Xte.shape[0]):\n",
    "            xi = Xte[i:i+1]\n",
    "            t0 = time.perf_counter()\n",
    "            _ = clf.predict_proba(xi)[:, 1]\n",
    "            t1 = time.perf_counter()\n",
    "            infer_times.append(t1 - t0)\n",
    "\n",
    "        it = np.asarray(infer_times, dtype=np.float64)\n",
    "        te_infer_timing = {\n",
    "            \"n_files\": int(len(it)),\n",
    "            \"total_s\": float(it.sum()),\n",
    "            \"mean_ms\": float(it.mean() * 1000.0),\n",
    "            \"p50_ms\": float(np.percentile(it, 50) * 1000.0),\n",
    "            \"p95_ms\": float(np.percentile(it, 95) * 1000.0),\n",
    "        }\n",
    "        te_total_timing = {\n",
    "            \"n_files\": int(Xte.shape[0]),\n",
    "            \"total_s\": float((te_feat_timing[\"total_s\"] if te_feat_timing else 0.0) + it.sum()),\n",
    "            \"mean_ms\": float(((te_feat_timing[\"total_s\"] if te_feat_timing else 0.0) + it.sum()) / Xte.shape[0] * 1000.0),\n",
    "            \"p50_ms\": float((te_feat_timing[\"p50_ms\"] if te_feat_timing else 0.0) + np.percentile(it, 50) * 1000.0),\n",
    "            \"p95_ms\": float((te_feat_timing[\"p95_ms\"] if te_feat_timing else 0.0) + np.percentile(it, 95) * 1000.0),\n",
    "        }\n",
    "\n",
    "    te_prob = clf.predict_proba(Xte)[:, 1]\n",
    "    te_pred = (te_prob >= best_thr).astype(int)\n",
    "\n",
    "    print(\"\\nBest threshold from validation:\", best_thr)\n",
    "    print(\"\\n=== TEST REPORT ===\")\n",
    "    print(classification_report(\n",
    "        yte, te_pred,\n",
    "        labels=[0, 1],\n",
    "        target_names=[\"Low\", \"High\"],\n",
    "        zero_division=0\n",
    "    ))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(yte, te_pred, labels=[0, 1]))\n",
    "    if len(np.unique(yte)) > 1:\n",
    "        print(\"ROC-AUC:\", roc_auc_score(yte, te_prob))\n",
    "\n",
    "    if measure_runtime:\n",
    "        _print_timing_block(\"TEST FEATURE EXTRACTION TIME (Cardiff features, stable)\", te_feat_timing)\n",
    "        _print_timing_block(\"TEST INFERENCE TIME (predict_proba)\", te_infer_timing)\n",
    "        _print_timing_block(\"TEST END-TO-END TIME (feature + inference)\", te_total_timing)\n",
    "\n",
    "    wrong = np.where(te_pred != yte)[0]\n",
    "    print(\"\\n=== MISCLASSIFIED TEST FILES ===\")\n",
    "    print(\"Total misclassified:\", len(wrong))\n",
    "    for i in wrong[:50]:\n",
    "        print(f\"- {pte[i]} | true={yte[i]} pred={te_pred[i]} pHigh={te_prob[i]:.3f}\")\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = train_and_evaluate(measure_runtime=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
